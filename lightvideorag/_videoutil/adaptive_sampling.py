import torch
import cv2
import decord
from decord import VideoReader, gpu
import kornia
import kornia.augmentation as K
import numpy as np
from kornia.constants import Resample
from moviepy.editor import VideoFileClip
from torchvision import transforms
from skimage.metrics import structural_similarity as ssim
from PIL import Image
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

fps_sampling_rate = 1  # é‡‡æ ·é¢‘ç‡ï¼ˆæ¯ç§’1å¸§ï¼‰
ssim_threshold = 0.8  # SSIM é˜ˆå€¼ï¼ˆå¤§äºæ­¤å€¼åˆ¤å®šä¸ºç›¸ä¼¼ï¼‰
segment_duration = 600  # æ¯ä¸ªåˆ†ç‰‡çš„é•¿åº¦ï¼ˆç§’ï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

data_transform = transforms.Compose(
    [
        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=(0.48145466, 0.4578275, 0.40821073),
            std=(0.26862954, 0.26130258, 0.27577711),
        ),
    ]
)

def similarity_sampling(video_path):
    # è¯»å–è§†é¢‘æ€»æ—¶é•¿
    overall_clip = VideoFileClip(video_path)
    duration = overall_clip.duration  # è§†é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
    num_segments = int(np.ceil(duration / segment_duration))  # è®¡ç®—åˆ†ç‰‡æ•°é‡

    overall_clip.reader.close()

    # è®¡ç®—æ€»å¸§æ•°
    total_frames = int(duration * fps_sampling_rate)

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è§†é¢‘ç‰‡æ®µ
    with tqdm(total=total_frames, desc="Processing Video", unit="frame") as progress_bar:
        with ThreadPoolExecutor() as executor:
            futures = {}
            for i in range(num_segments):
                start_time = i * segment_duration
                end_time = min((i + 1) * segment_duration, duration)
                futures[executor.submit(process_segment,video_path, start_time, end_time, i, progress_bar)] = i

            # æ”¶é›†ç»“æœï¼ŒæŒ‰é¡ºåºåˆå¹¶
            processed_tensors = []
            frame_timestamps = []  # å­˜å‚¨æ—¶é—´æˆ³
            for future in sorted(futures.keys(), key=lambda f: futures[f]):
                segment_index, tensors = future.result()
                for timestamp, tensor in tensors:
                    processed_tensors.append(tensor)
                    frame_timestamps.append(timestamp)

    # å †å å¼ é‡
    if processed_tensors:
        final_tensor = torch.stack(processed_tensors, dim=0)
        print(f"æœ€ç»ˆå¼ é‡å¤§å°: {final_tensor.shape}")  # (batch_size, 3, 224, 224)
    else:
        final_tensor = None
        print("æœªä¿å­˜ä»»ä½•å¸§ï¼")

    return frame_timestamps, final_tensor

def process_segment(video_path, start_time, end_time, segment_index, progress_bar):
    """å¤„ç†è§†é¢‘ç‰‡æ®µï¼Œå¹¶è¿”å›å·²å¤„ç†çš„å¼ é‡ã€‚"""
    clip = VideoFileClip(video_path).subclip(start_time, end_time)
    frame_times = np.arange(0, end_time - start_time, 1 / fps_sampling_rate)  # è®¡ç®—é‡‡æ ·æ—¶é—´ç‚¹

    prev_frame = None
    segment_tensors = []

    for t in frame_times:
        frame = clip.get_frame(t)  # è·å–æ—¶é—´ç‚¹ t å¤„çš„å¸§
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # è½¬æ¢ä¸ºç°åº¦

        if prev_frame is not None:
            similarity = ssim(prev_frame, gray_frame)
            if similarity > ssim_threshold:
                progress_bar.update(1)
                continue  # ä¸¢å¼ƒç›¸ä¼¼å¸§

        # è½¬æ¢ä¸º PIL.Image å¹¶é¢„å¤„ç†
        pil_image = Image.fromarray(frame)
        processed_image = data_transform(pil_image).to(device)
        segment_tensors.append((t + start_time, processed_image))
        progress_bar.update(1)
        prev_frame = gray_frame

    clip.reader.close()
    return segment_index, segment_tensors  # ç›´æ¥è¿”å›ï¼Œæ— éœ€æ’åº


### for retrieve
def filter_similar_frames(frames: list, timestamps: list, keep_frames: list, ssim_threshold=0.8):
    prev_frame = None
    filtered_frames = {}

    for idx, frame in enumerate(frames):
        timestamp = timestamps[idx]
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # è½¬æ¢ä¸ºç°åº¦

        # å¼ºåˆ¶ä¿ç•™ `keep_frames` é‡Œçš„å¸§ï¼Œæ— è®ºç›¸ä¼¼åº¦å¦‚ä½•
        if timestamp in keep_frames:
            pil_image = Image.fromarray(frame)
            filtered_frames[timestamp] = pil_image
            prev_frame = gray_frame
            continue  # ç›´æ¥è·³è¿‡ `SSIM` è®¡ç®—

        # ğŸš€ è®¡ç®— `SSIM`ï¼Œé«˜ç›¸ä¼¼åº¦çš„å¸§ä¸¢å¼ƒ
        if prev_frame is not None:
            similarity = ssim(prev_frame, gray_frame)
            if similarity > ssim_threshold:
                continue  # ä¸¢å¼ƒç›¸ä¼¼å¸§

        # è½¬æ¢ä¸º `PIL.Image` å¹¶ä¿å­˜
        pil_image = Image.fromarray(frame)
        filtered_frames[timestamp] = pil_image
        prev_frame = gray_frame

    return filtered_frames

##################GPU Version####################

def __build_gpu_transform(device):
    """ç­‰ä»·äº torchvision çš„ Resize(224) + CenterCrop + Normalize"""
    return torch.nn.Sequential(
        K.Resize(size=(224, 224), resample=Resample.BICUBIC, align_corners=False),
        K.Normalize(
            mean=torch.tensor([0.48145466, 0.4578275, 0.40821073], device=device),
            std=torch.tensor([0.26862954, 0.26130258, 0.27577711], device=device)
        )
    )


def __get_adaptive_window_size(h, w, min_size=11, max_size=51):
    short_side = min(h, w)
    win = short_side // 20  # ä¾‹å¦‚ï¼š480 é«˜çš„å¸§ â†’ 480/20=24
    win = max(min_size, min(win, max_size))  # é™å®šèŒƒå›´
    if win % 2 == 0:
        win += 1  # ç¡®ä¿æ˜¯å¥‡æ•°ï¼ˆSSIMè¦æ±‚ï¼‰
    return win


def similarity_sampling_kornia_gpu(
    video_path,
    device=torch.device("cuda:0")
):
    from decord import bridge
    bridge.set_bridge("torch")

    # åˆå§‹åŒ–è§†é¢‘è¯»å–
    vr = VideoReader(video_path, ctx=gpu(0))
    native_fps = vr.get_avg_fps()
    total_frames = len(vr)

    # å¸§é—´æ­¥é•¿
    step = int(round(native_fps / fps_sampling_rate)) if fps_sampling_rate < native_fps else 1
    sampled_indices = list(range(0, total_frames, step))
    sampled_indices = sampled_indices[:-1]
    if len(sampled_indices) > 60:
        sampled_indices = sampled_indices[:-2]
    if len(sampled_indices) > 300:
        sampled_indices = sampled_indices[:-2]

    # GPU é¢„å¤„ç† transform
    gpu_transform = __build_gpu_transform(device)

    all_frames = []
    all_timestamps = []
    prev_frame_tensor = None

    frame_tensor = vr[0]
    h, w = frame_tensor.shape[0], frame_tensor.shape[1]
    window_size = __get_adaptive_window_size(h, w)
    # print(f"[Auto SSIM] åˆ†è¾¨ç‡: {w}x{h}, ä½¿ç”¨ window_size={window_size}")
    with tqdm(total=len(sampled_indices), desc="Processing", unit="frame") as pbar:
        for frame_idx in sampled_indices:
            try:
                # è·å–å¸§: [H, W, 3], uint8, CUDA
                frame_tensor = vr[frame_idx]  # torch.Tensor

                # è½¬ä¸º [1, 3, H, W] float32
                frame_for_ssim = frame_tensor.permute(2, 0, 1).unsqueeze(0).float() / 255.0  # [1,3,H,W]

                if prev_frame_tensor is not None:
                    ssim_val = kornia.metrics.ssim(
                        prev_frame_tensor, frame_for_ssim,
                        window_size=window_size
                    ).mean()

                    if ssim_val.item() > ssim_threshold:
                        pbar.update(1)
                        continue

                # åº”ç”¨ transform â†’ é¢„å¤„ç†ä¸º [3, 224, 224]
                processed = gpu_transform(frame_for_ssim).squeeze(0)
                all_frames.append(processed)
                all_timestamps.append(round(frame_idx / native_fps,3))

                prev_frame_tensor = frame_for_ssim
                pbar.update(1)

            except Exception as e:
                print(f"[è·³è¿‡] å¤„ç†å¸§ {frame_idx} å‡ºé”™: {e}")
                pbar.update(1)
                continue

    if all_frames:
        final_tensor = torch.stack(all_frames, dim=0)  # [N,3,224,224]
        print(f"âœ… ä¿ç•™å¸§æ•°: {final_tensor.shape[0]}, å°ºå¯¸: {final_tensor.shape}")
    else:
        final_tensor = None
        print("âŒ æœªä¿ç•™ä»»ä½•å¸§")

    return all_timestamps, final_tensor



def filter_similar_frames_kornia_gpu(
    frames: list,                   # List[np.ndarray]ï¼Œæ¯ä¸ªæ˜¯ [H, W, 3]ï¼Œuint8
    timestamps: list,              # List[float]ï¼Œæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    keep_frames: list = [],        # å¼ºåˆ¶ä¿ç•™çš„æ—¶é—´æˆ³
    ssim_threshold: float = 0.9,
    window_size: int = None,
    device: torch.device = torch.device("cuda:0")
):
    assert len(frames) == len(timestamps)

    filtered_frames = {}
    prev_tensor = None

    if window_size is None and len(frames) > 0:
        h, w = frames[0].shape[:2]
        window_size = __get_adaptive_window_size(h, w)
        # print(f"[Auto SSIM] åˆ†è¾¨ç‡: {w}x{h} â†’ window_size={window_size}")

    for idx, frame_np in enumerate(frames):
        timestamp = timestamps[idx]

        # è½¬ä¸º [1, 3, H, W] float tensor
        frame_tensor = torch.from_numpy(frame_np).permute(2, 0, 1).unsqueeze(0).float() / 255.0
        frame_tensor = frame_tensor.to(device)

        # å¼ºåˆ¶ä¿ç•™å¸§
        if timestamp in keep_frames:
            image = Image.fromarray(frame_np)
            filtered_frames[timestamp] = image
            prev_tensor = frame_tensor
            continue

        # Kornia SSIM åˆ¤æ–­
        if prev_tensor is not None:
            ssim_val = kornia.metrics.ssim(prev_tensor, frame_tensor, window_size=window_size).mean()
            if ssim_val.item() > ssim_threshold:
                continue  # è·³è¿‡ç›¸ä¼¼å¸§

        # ä¿ç•™åŸå›¾
        image = Image.fromarray(frame_np)
        filtered_frames[timestamp] = image
        prev_tensor = frame_tensor

    return filtered_frames

if __name__ == "__main__":
    # similarity_sampling(
    #     "../../testVideo/HarryPotterM1.mkv")

    similarity_sampling_kornia_gpu("../../testVideo/HarryPotterM1.mkv")